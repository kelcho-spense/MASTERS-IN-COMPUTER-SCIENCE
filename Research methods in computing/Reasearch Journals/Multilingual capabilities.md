# Literature Review: Cross-Lingual Transfer Learning and Multilingual NLP Models

Problem Statement: As NLP models are predominantly trained on languages with abundant data (like English), there's a growing challenge to extend these capabilities to low-resource languages. How can we develop systems that effectively transfer learning from high-resource languages (like English) to low-resource languages, considering differences in grammar, vocabulary, and cultural context?

| Articles + References                                                                                                                                                                                                                                | Year | Definition of Key terms & concepts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Research methods                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Analysis                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Summary of research results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Research Gap                                                                                                                                                                                                                                                                                                                                                                 |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| MEGA: Multilingual Evaluation of Generative AI`<br><br>`Authors: Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, Sunayana Sitaram | 2023 | •**Generative AI Models**: AI systems like GPT-3.5 and GPT-4, capable of generating text based on given prompts.`<br><br>`• **Multilingual Capabilities**: The ability of AI models to understand and generate text in multiple languages.`<br><br>`• **Benchmarking**: The process of evaluating AI models using standardized tasks and datasets.`<br><br>`• **Low-Resource Languages**: Languages with limited training data or computational resources, often leading to poorer model performance.`<br><br>`• **Prompting Strategies**: Methods used to guide AI models during evaluation, such as machine translation (translate-test), zero-shot, and few-shot learning.                                                                                                                                                                     | •**Dataset**: The study uses 16 NLP datasets, covering tasks like classification, question answering, sequence labeling, and natural language generation, across 70 typologically diverse languages.`<br><br>`• **Models Evaluated**: GPT-3.5, GPT-4, BLOOMZ, and SOTA fine-tuned models like TULRv6 and MuRIL are evaluated for their multilingual capabilities.`<br><br>`• **Evaluation Framework**: MEGA evaluates performance across various tasks and languages, comparing generative models with fine-tuned models using multiple prompting strategies.`<br><br>`• **Language Distribution**: The study spans a wide array of languages, including high-resource and low-resource languages, covering various language families. | •**Multilingual Performance**: The study highlights a significant performance gap between high-resource languages (like English) and low-resource languages, especially those with non-Latin scripts. Fine-tuned models generally outperform generative models on low-resource languages.`<br><br>`• **Challenges**: Translating input into English (translate-test strategy) often yields better results than using non-English languages directly. Performance depends heavily on prompting quality and dataset contamination.`<br><br>`• **Prompting Strategies**: Translate-test prompting showed improvements, particularly in low-resource languages. The study also investigates how factors like few-shot examples affect performance. | •**Disparities in Performance**: While GPT-4 has improved performance compared to previous models, it still struggles with low-resource languages, particularly those with non-Latin scripts. Fine-tuned models tend to outperform generative models on multilingual tasks.`<br><br>`• **Evaluation Frameworks**: The study establishes MEGA as a robust framework for evaluating generative AI in a multilingual context, offering strategies to address multilingual performance disparities.`<br><br>`• **Future Work**: The research emphasizes the need for broader multilingual benchmarks covering more underrepresented languages and improved alignment strategies for multilingual models. | The study points out the need for comprehensive multilingual evaluations of LLMs beyond high-resource languages, focusing on low-resource languages that are underrepresented in current benchmarks. Additionally, there is a need to better understand the impact of different prompting strategies on the performance of generative models across diverse languages.       |
| METAL: Towards Multilingual Meta-Evaluation`<br><br>`Authors: Rishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, Sunayana Sitaram                                                                                                               | 2024 | •**Meta-Evaluation**: The evaluation of large language models (LLMs) using another LLM to assess performance across different tasks and languages.`<br><br>`• **LLM Evaluators**: LLMs like GPT-3.5-Turbo, GPT-4, and PaLM2 used as evaluators for subjective metrics.`<br><br>`• **Summarization**: The NLP task of generating concise summaries from larger texts.`<br><br>`• **Metrics**: Different evaluation measures like Linguistic Acceptability (LA), Output Content Quality (OCQ), and Task Quality (TQ) for assessing summary quality.`<br><br>`• **Bias and Alignment**: Discrepancies between human judgments and LLM-generated evaluations, particularly in multilingual contexts.                                                                                                                                                  | •**Dataset**: METAL dataset, created specifically for evaluating LLMs in multilingual summarization tasks, with 1000 summaries across 10 languages (English, French, Chinese, Hindi, Arabic, Bengali, Russian, Turkish, Japanese, and Swahili).`<br><br>`• **LLM Evaluators**: GPT-3.5-Turbo, GPT-4, and PaLM2 were used as evaluators on the dataset.`<br><br>`• **Annotation**: The summaries were annotated by human evaluators along five metrics: Linguistic Acceptability (LA), Output Content Quality (OCQ), Task Quality (TQ), Problematic Content (PC), and Hallucinations (H).`<br><br>`• **Prompting Strategy**: Simple and detailed prompting strategies were used to assess the LLMs' performance across various metrics. | •**LLM Performance**: GPT-4 and PaLM2 performed better than GPT-3.5-Turbo as evaluators across languages, but their reasoning often did not match human evaluations.`<br><br>`• **Bias**: The study highlights the challenges posed by biases in LLMs, particularly when the model was prompted in languages with limited training data.`<br><br>`• **Multilingual Challenges**: The results show that while LLMs can perform well in multiple languages, their evaluations can still be inconsistent when compared to human judgments, particularly in low-resource languages.                                                                                                                                                                | •**Performance Differences**: GPT-4 outperformed GPT-3.5-Turbo and PaLM2 when evaluated on summarization tasks, but all LLM evaluators showed a mismatch in reasoning compared to human evaluators.`<br><br>`• **Multilingual Evaluation**: The study provides a framework for using LLMs as evaluators across various languages and tasks, revealing that different languages pose unique challenges for LLMs in terms of bias and accuracy.`<br><br>`• **Bias and Alignment**: The analysis showed that while LLMs can generate human-like evaluations, their reasoning may be influenced by biases related to the language of the input and training data.                                        | The study reveals a significant gap in the alignment of LLMs when evaluating non-English languages. There is a need for more robust datasets and methods to align LLMs with human evaluations, especially in low-resource languages, to improve evaluation consistency.                                                                                                      |
| Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained`<br><br>`Authors: Nora Kassner, Philipp Dufter, Hinrich Schütze                                                                                                             | 2021 | •**Multilingual Pretrained Models**: Models like mBERT trained on multilingual data for cross-lingual tasks.`<br><br>`• **LAMA**: A framework that queries pretrained language models using masked language models to test knowledge across languages and tasks.`<br><br>`• **Typed Querying (TyQ)**: A technique to avoid errors caused by language-specific morphology by using a defined set of possible answers for each relation.`<br><br>`• **Bias**: The phenomenon where language models show performance biases when queried in different languages.`<br><br>`• **Pooling**: A method to combine predictions from multiple languages to improve performance by eliminating language-specific biases.                                                                                                                                     | •**Dataset**: The study uses TREx and GoogleRE, which consist of triples (object, relation, subject), and translates them into 53 languages for evaluating multilingual performance.`<br><br>`• **Querying Techniques**: The study explores both Untyped Querying (UnTyQ) and Typed Querying (TyQ). TyQ uses a predefined set of candidate answers for better handling of language-specific challenges.`<br><br>`• **Models Evaluated**: mBERT, fine-tuned monolingual BERT, and other multilingual pretrained models like XLM-R are tested on the queries.`<br><br>`• **Evaluation**: Performance is measured using precision at one (p1) to evaluate knowledge retrieval accuracy across multiple languages.                         | •**Language Performance**: mBERT, although trained on 104 languages, shows varying performance across languages. It works well for some languages like English, but poorly for others, such as Japanese and Thai.`<br><br>`• **Bias and Language Specificity**: mBERT exhibits biases when queried in certain languages (e.g., Italian queries often yield Italy as the country of origin). Pooling predictions across languages helps reduce these biases.`<br><br>`• **Pooling**: Pooling predictions across multiple languages results in better performance, even outperforming monolingual BERT when applied to the LAMA-UHN task.                                                                                                        | •**Performance Gaps**: While mBERT performs reasonably well in 21 languages, its performance drops below 60% for 32 other languages. This highlights that mBERT does not store knowledge in a language-independent manner.`<br><br>`• **Effectiveness of TyQ**: TyQ querying proved more effective than UnTyQ, especially for tasks requiring multi-token answers. This method significantly improves knowledge retrieval accuracy.`<br><br>`• **Pooling Benefits**: Pooling predictions across languages addresses the language-specific performance gap, making mBERT more robust in multilingual knowledge retrieval tasks.                                                                       | The study suggests that while multilingual models like mBERT perform reasonably well, there is still a significant gap in performance between high-resource and low-resource languages. Further research is needed to address these disparities and improve language-independent knowledge storage in pretrained models.                                                     |
| Cross-Lingual Transfer for Low-Resource Natural Language Processing`<br><br>`Author: Iker García-Ferrero                                                                                                                                          | 2025 | •**Cross-lingual Transfer Learning**: A technique where knowledge from high-resource languages is applied to improve NLP tasks in low-resource languages, using data-based or model-based methods.`<br><br>`• **Sequence Labeling Tasks**: NLP tasks involving assigning labels to each word in a text sequence (e.g., NER, Opinion Target Extraction, Argument Mining).`<br><br>`• **Data-based Transfer**: Method utilizing parallel data or machine translation to project annotations from high-resource to low-resource languages.`<br><br>`• **Model-based Transfer**: Method using pre-trained multilingual models for direct application to low-resource languages without additional annotated data.`<br><br>`• **Annotation Projection**: Process of transferring annotations between languages through translation or word alignments. | •**T-Projection Method**: Improves data-based transfer using state-of-the-art annotation projection techniques with text-to-text multilingual models and machine translation systems. Automates annotation projection from high-resource to low-resource languages.`<br><br>`• **Constrained Decoding Algorithm**: A model-based transfer approach enhancing cross-lingual sequence labeling in zero-shot settings, utilizing pre-trained multilingual models like mT5.`<br><br>`• **Medical mT5**: Development of the first multilingual text-to-text model for the medical domain, providing an open-source solution for cross-lingual transfer in specialized fields.                                                                        | •**Effectiveness of Techniques**: The T-Projection method demonstrates significant improvements over baseline methods for cross-lingual transfer, particularly for sequence labeling tasks in low-resource scenarios.`<br><br>`• **Domain Adaptability**: The research shows that specialized domains like medical NLP can benefit from cross-lingual transfer, with Medical mT5 outperforming general-purpose multilingual models on medical tasks.`<br><br>`• **Zero-shot Performance**: The constrained decoding algorithm enables better zero-shot transfer, addressing challenges in sequence labeling tasks without target language annotations.                                                                                         | •**Improved Transfer Methods**: The study establishes more effective techniques for transferring knowledge from high-resource to low-resource languages, particularly for sequence labeling tasks.`<br><br>`• **Domain-specific Solutions**: Medical mT5 demonstrates the potential of specialized multilingual models for domain-specific applications, providing better performance than general-purpose models in medical contexts.`<br><br>`• **Practical Applications**: The techniques developed show promise for real-world applications in environments where annotated data is scarce, offering solutions that require minimal resources in target languages.                               | While significant advancements are made in cross-lingual transfer for sequence labeling tasks, challenges remain in adapting these techniques to more complex NLP tasks and extremely low-resource languages. Further research is needed to develop methods that can effectively transfer knowledge across typologically diverse language families with minimal supervision. |
