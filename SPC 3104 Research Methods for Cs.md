



| Title                                                   | Authors                                                                                                                                                     | Year of Publication | Key Terms & Concepts                                                                                                               | Research Methods                                                                                                                                  | Analysis                                                                                                        | Summary of Research Results                                                                                                                                   | Research Gap                                                                                                                                                      |
|---------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------|-----------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| How Vocabulary Sharing Facilitates Multilingualism in LLaMa | Fei Yuan, Shuai Yuan, Zhiyong Wu, Lei Li                                                                                                                     | 2024                | Multilingual Capability, Vocabulary Sharing, Embedding Fine-Tuning, Quadrants                                                         | Bilingual instruction translation datasets, Embed FT method, 101 language pairs, LLaMa models                                                    | Indo-European languages benefit from higher multilingual performance, TyQ outperforms UnTyQ, Over-tokenization challenge | Embedding Fine-Tuning significantly boosts multilingual performance in Indo-European languages, pooling predictions reduces bias                      | Need for further research into migration between quadrants, better methods for handling over-tokenization in low-resource languages                               |
| MEGA: Multilingual Evaluation of Generative AI           | Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, Sunayana Sitaram | 2023                | Generative AI Models, Multilingual Capabilities, Benchmarking, Low-Resource Languages, Prompting Strategies                              | 16 NLP datasets across 70 languages, GPT-3.5, GPT-4, and SOTA models, Translate-test and zero-shot strategies                                    | Performance gap between high-resource and low-resource languages, Translate-test improves low-resource performance, Fine-tuned models outperform generative models | GPT-4 and PaLM2 exhibit strong performance but show discrepancies in low-resource languages, fine-tuned models are more effective                      | Further development of multilingual benchmarks, better alignment strategies for low-resource languages, more diverse evaluation methods |
| METAL: Towards Multilingual Meta-Evaluation              | Rishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, Sunayana Sitaram                                                                                      | 2024                | Meta-Evaluation, LLM Evaluators, Summarization, Bias and Alignment, Multilingual Evaluation                                              | METAL dataset, GPT-3.5-Turbo, GPT-4, PaLM2 as evaluators, Human annotation for summarization tasks                                              | GPT-4 and PaLM2 outperform GPT-3.5-Turbo, High consistency for non-English languages but bias in low-resource languages | MLLMs' evaluators align well with human judgments for high-resource languages, but misalignments persist in low-resource contexts                      | More robust multilingual evaluation datasets, better alignment between LLM-generated evaluations and human judgments |
| Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained | Nora Kassner, Philipp Dufter, Hinrich Schütze                                                                                                               | 2021                | Multilingual Pretrained Models, LAMA, Bias, Typed Querying, Cross-Lingual Transfer                                                       | TREx and GoogleRE datasets, 53 languages, mBERT, XLM-R, TyQ method                                                                           | MLLMs perform better with certain languages, TyQ improves performance in cross-lingual tasks, Biases in model outputs     | mBERT performs well in high-resource languages, but performance is inconsistent in low-resource languages, TyQ improves accuracy                        | Enhancing multilingual performance in low-resource languages, more research on effective multilingual training strategies |
| Multilingual Large Language Models: A Systematic Survey   | Shaolin Zhu, Supryadi, Shaoyang Xu, Haoran Sun, Leiyu Pan, Menglong Cui, Jiangcun Du, Renren Jin, António Branco, Deyi Xiong                                 | 2024                | MLLMs, Cross-Lingual Transfer, Pre-training, Fine-Tuning, Alignment, Cultural Adaptation, Ethical AI                                    | Multilingual corpora, Transformer and MoE architectures, Pre-training with multilingual datasets, Multilingual evaluation benchmarks               | MLLMs excel in high-resource languages but struggle in low-resource languages, Cross-lingual transfer improves multilingual performance | MLLMs show impressive cross-lingual transfer, but performance gaps remain in low-resource languages, further fine-tuning needed                      | Inclusive datasets for low-resource languages, more work on cross-lingual knowledge transfer, improving alignment with human values |

